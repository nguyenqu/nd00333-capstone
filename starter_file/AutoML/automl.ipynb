{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "## Import libraries for Azure Machine Learning SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import joblib\n",
    "import logging\n",
    "import sklearn\n",
    "import pkg_resources\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Model\n",
    "\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep\n",
    "from azureml.contrib.pipeline.steps import ParallelRunConfig\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "print('Workspace name:\\t'  + ws.name,\n",
    "      'Resource group:\\t'  + ws.resource_group,\n",
    "      'Azure region:\\t'    + ws.location,\n",
    "      'Subscription id:\\t' + ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for experiment\n",
    "experiment_name = 'automl-heart-failure-experiment'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "run = experiment.start_logging()\n",
    "\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach an AmlCompute Target\n",
    "We will need to create a compute target for our AutoML run. We will use ***vm_size = Standard_DS3_v2*** in our provisioning configuration and select ***max_nodes*** to be no greater than 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Name for the CPU cluster\n",
    "amlcompute_cluster_name = \"automl-cpu-compute-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    amlcompute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    amlcompute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2', max_nodes=4)\n",
    "    amlcompute_target = ComputeTarget.create(ws, amlcompute_cluster_name, amlcompute_config)\n",
    "\n",
    "amlcompute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_targets = ws.compute_targets\n",
    "\n",
    "for i, key in enumerate(compute_targets):\n",
    "    print(f\"{i+1}. Compute target\\n\\tname: {compute_targets[key].name}\\n\\tType: {compute_targets[key].type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a more detailed view of current AmlCompute status, use get_status().\n",
    "print(amlcompute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "The dataset used for this project is the ***Heart Failure Clinical Records*** dataset, which can be found [here](https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records) in the UCI Machine Learning Repository. \n",
    "\n",
    "This dataset contains the medical records of 299 patients who had heart failure, collected during their follow-up period, where each patient profile has 13 clinical features.\n",
    "\n",
    "The task we are concerned with is to predict whether the patient died during the follow-up period. We will target the DEATH_EVENT column and since it is a boolean variable, the task is binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# Try to load the dataset from the Workspace. Otherwise, create it from the file\n",
    "description_text = \"Health Failure dataset from UCI ML-Repository for mortality prediction for the Capstone Project.\"\n",
    "key = \"HealthFailure Dataset\"      # the key to match the dataset name\n",
    "\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv\"\n",
    "\n",
    "if key in ws.datasets.keys():\n",
    "    dataset = ws.datasets[key]\n",
    "    print(\"The Dataset was found!\")\n",
    "else:\n",
    "    dataset = Dataset.Tabular.from_delimited_files(dataset_url) # Create AML Dataset and register it into Workspace\n",
    "    dataset = dataset.register(workspace=ws, name=key, description=description_text) # Register Dataset in Workspace\n",
    "\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the datasets for the Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "\n",
    "# Split the dataset into training and testing datasets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Save training data to csv file\n",
    "train_df.to_csv(\"./data/train_data.csv\", index=False)\n",
    "\n",
    "# Read saved training data and create a dataset in Azure ML\n",
    "data_store = ws.get_default_datastore()\n",
    "data_store.upload(src_dir=\"./data\", target_path=\"training_data\")\n",
    "train_ds = TabularDatasetFactory.from_delimited_files(path=[(data_store, 'training_data/train_data.csv')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the Training Dataset Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.take(5).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "- ***experiment_timeout_minutes = 20***: Specifies how long (in minutes) our experiment should run. In previous projects we could not set more than 30 minutes. We could use more in this project, but it's not needed for such a small training set. To reduce the time taken to train, experiment_timeout_minutes of 20 was chosen.\n",
    "\n",
    "- ***max_concurrent_iterations = 4***: The maximum number of iterations that could be run in parallel. It is recommended to create a dedicated cluster per experiment and adjust the number of max_concurrent_iterations of your experiment to the number of nodes in the cluster. In this way you use all nodes of the cluster at the same time with the desired number of concurrent child runs/iterations. So I set the value to 4.\n",
    "\n",
    "- ***primary_metric = 'accuracy'***: The metric that is optimized by automated machine learning for model selection. We have set the \"accuracy\"/\"AUC_weighted\".\n",
    "\n",
    "- ***compute_target = amlcompute_target*** : The compute target with specific vm_size and max_nodes used to run the experiment. The local compute was chosen as this may be slower but generally provides better results.\n",
    "\n",
    "- ***task = 'classification'*** : We have a classification task to do, we want to predict whether the person will have heart failure or not. In other words, we're trying to predict the DEATH_EVENT.\n",
    "\n",
    "- ***training_data = train_ds*** : The data (80% of the total dataset) on which used in the experiment to train the algorithm.\n",
    "\n",
    "- ***label_column_name = \"DEATH_EVENT\"*** : The target variable to predict.\n",
    "\n",
    "- ***path = project_folder*** : The full path to the Azure ML folder of the project './capstone-project'.\n",
    "\n",
    "- ***enable_early_stopping = True*** : Early stopping is enabled so if a run is not performing well, it can stop early, again to save time and if not performing well continuing seems uncessary.\n",
    "\n",
    "- ***featurization = 'auto'*** : indicator of whether the featurization step should be performed automatically or not, or whether a custom featurization should be used. I used \"Auto\" so the featurization step should be automatic.\n",
    "\n",
    "- ***debug_log = \"automl_errors.log\"*** : The debug information are written to the automl_errors.log.\n",
    "\n",
    "- ***enable_onnx_compatible_models = False*** : Whether to enable or disable enforcing the ONNX-compatible models.\n",
    "\n",
    "- ***blocked_models = ['XGBoostClassifier']*** : What algorithm we want from AutoML to not run. I selected XGBoostClassifier, the answer could be found in the forum (Link). For those who don't have access, I have to say that it is for compatibility issues. So the lack of time to make the  XGBoostClassifier to run make me to enforce the AutoML to not run this specific algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "project_folder = './capstone-project'\n",
    "\n",
    "# Define automl settings\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 20,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"primary_metric\" : 'AUC_weighted'\n",
    "}\n",
    "\n",
    "# Define automl configuration settings\n",
    "automl_config = AutoMLConfig(compute_target = amlcompute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data = train_ds,\n",
    "                             label_column_name = \"DEATH_EVENT\",   \n",
    "                             path = project_folder,\n",
    "                             enable_early_stopping = True,\n",
    "                             featurization = 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             enable_onnx_compatible_models = False,    # --> Addition\n",
    "                             blocked_models = ['XGBoostClassifier'], # --> Addition\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Submit the experiment to the compute target \n",
    "remote_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "Use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(remote_run).show()\n",
    "for children_run in remote_run.get_children():\n",
    "    print('-----------------------------------')\n",
    "    print(children_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "Get the best model from the automl experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get the best model\n",
    "best_run, best_model = remote_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# parameter details of the best model\n",
    "def print_model(model, prefix=\"\"):\n",
    "    for step in model.steps:\n",
    "        print(prefix + step[0])\n",
    "        if hasattr(step[1], 'estimators') and hasattr(step[1], 'weights'):\n",
    "            pprint({'estimators': list(\n",
    "                e[0] for e in step[1].estimators), 'weights': step[1].weights})\n",
    "            print()\n",
    "            for estimator in step[1].estimators:\n",
    "                print_model(estimator[1], estimator[0] + ' - ')\n",
    "        else:\n",
    "            pprint(step[1].get_params())\n",
    "            print()\n",
    "\n",
    "print_model(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for primary_metric in best_run.get_metrics():\n",
    "    metric=best_run_metrics[primary_metric]\n",
    "    print(primary_metric,metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_metrics(name='AUC_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_properties()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into x and y tests\n",
    "y_test = test_df['DEATH_EVENT']\n",
    "x_test = test_df.drop(['DEATH_EVENT'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Test the best model and create a confusion matrix\n",
    "ypred = best_model.predict(x_test)\n",
    "cmatrix = confusion_matrix(y_test, ypred)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "##pd.DataFrame(cmatrix)\n",
    "pd.DataFrame(cmatrix).style.background_gradient(cmap='Blues', low=0, high=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.register_model(model_name='best_run_automl', model_path='./outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19064\\3453546442.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create inference folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'inference'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "\n",
    "# create inference folder\n",
    "inference_folder = 'inference'\n",
    "if inference_folder not in os.listdir():\n",
    "    os.mkdir(inference_folder)\n",
    "\n",
    "# Save the best model\n",
    "##joblib.dump(best_model, filename = inference_folder + '/best_automl_model.joblib')\n",
    "##joblib.dump(value=best_model, filename='output/best-automl.pkl')\n",
    "\n",
    "# Save the best model, scoring script, and conda env files in inference folder\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', inference_folder + '/best_automl_score.py')\n",
    "best_run.download_file('outputs/model.pkl', inference_folder + '/best_automl_model.pkl')\n",
    "\n",
    "best_run.download_file('outputs/conda_env_v_1_0_0.yml', inference_folder + 'automl_conda_env.yml')\n",
    "##best_run.download_file(constants.CONDA_ENV_FILE_PATH, inference_folder + 'automl_conda_env.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of environments\n",
    "Environment.list(workspace=ws).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the environment\n",
    "my_env = Environment.get(workspace=ws, name=\"AzureML-AutoML\")\n",
    "my_env.save_to_directory('env', overwrite=True)\n",
    "\n",
    "my_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "Register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Register the model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "model_name = best_run.properties['model_name']\n",
    "local_file = inference_folder + '/best_automl_model.pkl'\n",
    "\n",
    "run_id = best_run.id\n",
    "experiment_name = best_run.experiment.name\n",
    "\n",
    "model = Model.register(workspace = ws,\n",
    "                       model_name = model_name,                        # Name of the registered model in your workspace.\n",
    "                       model_path = local_file,                        # Local file to upload and register as a model.\n",
    "                       model_framework = Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n",
    "                       model_framework_version = sklearn.__version__,  # Version of scikit-learn used to create the model.\n",
    "                       description = 'Best autoML model to predict motality caused by heart failure.',\n",
    "                       tags={'area': 'heart-failure', 'type': 'classification'})\n",
    "\n",
    "print('Model name:', model.name)\n",
    "print('Model id:', model.id)\n",
    "print('Model version:', model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference configuration\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "env = Environment.from_conda_specification(name=\"my_env\", file_path=inference_folder + 'automl_conda_env.yml')\n",
    "env\n",
    "#### Siehe Oben my_env --> Ist es gleich?\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=inference_folder + '/best_automl_score.py', environment=env)\n",
    "\n",
    "# display the environment file\n",
    "with open(file_path=inference_folder + 'automl_conda_env.yml', 'r') as file:\n",
    "    env_file = file.read()\n",
    "    print(env_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Deployment\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# define deployment configuration\n",
    "aci_deployment_config = AciWebservice.deploy_configuration(cpu_cores=1,\n",
    "                                                           memory_gb=1,\n",
    "                                                           tags={'area': \"heart-failure\", 'type': \"classification\"},\n",
    "                                                           description=\"Predict heart failure mortality using classification model\",\n",
    "                                                           auth_enabled=True,\n",
    "                                                           enable_app_insights=True)\n",
    "\n",
    "# deploy model as webservice using Azure Container Instance(ACI)\n",
    "aci_service = Model.deploy(workspace = ws, \n",
    "                           name = \"aci-heart-failure-deploy\", \n",
    "                           models = [model], \n",
    "                           inference_config = inference_config, \n",
    "                           deployment_config = aci_deployment_config, \n",
    "                           overwrite=True)\n",
    "\n",
    "aci_service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the active api endpoint for scoring\n",
    "print(f\"Service State: {aci_service.state}\\n\")\n",
    "print(f\"Scoring URI:   {aci_service.scoring_uri}\\n\")\n",
    "print(f\"Swagger URI:   {aci_service.swagger_uri}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Consuming the model\n",
    "Send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to the web service\n",
    "import json\n",
    "import requests\n",
    "\n",
    "'''\n",
    "input_data = json.dumps({\n",
    "    \"data\": [\n",
    "            [75.0, 0.0, 582.0, 0.0, 20.0, 1.0, 265000.0, 1.9, 130.0, 1.0, 0.0, 4.0],\n",
    "            [80.0, 1.0, 123.0, 0.0, 35.0, 1.0, 388000.0, 9.4, 133.0, 1.0, 1.0, 10.0],\n",
    "            [62.0, 0.0, 61.0, 1.0, 38.0, 1.0, 155000.0, 1.1, 143.0, 1.0, 1.0, 270.0],\n",
    "            [50.0, 1.0, 111.0, 0.0, 20.0, 0.0, 210000.0, 1.9, 137.0, 1.0, 0.0, 7.0]\n",
    "        ]\n",
    "    })\n",
    "'''\n",
    "\n",
    "# 4 sets of data to score, so we get two results back\n",
    "test_sample = test_df.sample(n=4)\n",
    "labels = test_sample.pop('DEATH_EVENT')\n",
    "\n",
    "\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps({\"data\": test_sample.to_dict(orient='records')})\n",
    "with open(\"input_data.json\", 'w') as _f:\n",
    "    _f.write(input_data)\n",
    "\n",
    "print(input_data)\n",
    "\n",
    "response = requests.post(aci_service.scoring_uri, data=input_data, headers={'Content-Type':'application/json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predictions from Service: {response.json()}\\n\")\n",
    "print(f\"Data Labels: {labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Print the log of the webservice\n",
    "print(aci_service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the webservice, model, and shut down the compute cluster\n",
    "aci_service.delete()\n",
    "model.delete()\n",
    "amlcompute_target.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
